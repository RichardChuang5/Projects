{"nbformat_minor": 0, "cells": [{"source": "# Guided Capstone PySpark", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"source": "#### Start a simple Spark Session", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": 1, "cell_type": "code", "source": "from pyspark.sql import SparkSession, SQLContext\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, DecimalType\nimport json, decimal\n\nspark = SparkSession.builder.master(\"local\").appName(\"app\").getOrCreate()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>40</td><td>application_1628529429212_0044</td><td>pyspark3</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-guided.quamnvr1jr2efhmihekrfcv21d.bx.internal.cloudapp.net:8088/proxy/application_1628529429212_0044/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn4-guided.quamnvr1jr2efhmihekrfcv21d.bx.internal.cloudapp.net:30060/node/containerlogs/container_1628529429212_0044_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "SparkSession available as 'spark'.\n"}], "metadata": {"cell_status": {"execute_time": {"duration": 957.638916015625, "end_time": 1628656056942.574}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 2, "cell_type": "code", "source": "# conf.set is used to input your key value pairs.\nspark.conf.set(\n\"fs.azure.account.key.adfrcdevstorage.blob.core.windows.net\",\n\"xrbJuOZpG1+WGZk2yJhbNbaIjB+adtF6d8BN+3xLnYMEDtwA+B3fzi44qPaPNpr8OJgzFXehzBQFC4/ZqX6vCQ==\"\n)", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 34.47802734375, "end_time": 1628656056989.206}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 3, "cell_type": "code", "source": "#csv function\ndef parse_csv(line):\n    record_type_pos = 2\n    record = line.split(\",\")\n    try:\n        # [logic to parse records]\n        if record[record_type_pos] == \"T\":\n            event = [record[0], record[1], record[2], record[3], record[4], int(record[5]), record[6],\n                     decimal.Decimal(record[7]), int(record[8]), None, None,\"T\"]\n            return event\n        elif record[record_type_pos] == \"Q\":\n            event = [record[0], record[1], record[2], record[3], record[4], int(record[5]), record[6],\n                     decimal.Decimal(record[7]), int(record[8]), decimal.Decimal(record[9]),int(record[10]), \"Q\"]\n            return event\n    \n    except Exception as e:\n        event = [None, None, None, None, None, None, None, None, None, None, None, \"B\"]\n        return event\n        ", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 34.288818359375, "end_time": 1628656057075.425}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 4, "cell_type": "code", "source": "# JSON Function\ndef parse_json(record):\n    try:\n        #record = json.loads(line)\n        record_type = record[\"event_type\"]\n        # logic to parse records\n        if record_type == \"T\":\n            #Get the applicable field values from json\n            event = [record['trade_dt'], record['file_tm'], record['event_type'], record['symbol'],record['event_tm'],\n                     int(record['event_seq_nb']),record['exchange'],None ,None,decimal.Decimal(record['price']), int(record['size']),'T']\n            return event\n        elif record_type == \"Q\":\n            # [Get the applicable field values from json]\n            event = [record['trade_dt'], record['file_tm'], record['event_type'], record['symbol'],record['event_tm'],\n                     int(record['event_seq_nb']),record['exchange'],decimal.Decimal(record['bid_pr']),int(record['bid_size']),\n                     decimal.Decimal(record['ask_pr']), int(record['ask_size']), 'Q']\n            return event\n        return record\n    except Exception as e:\n    # [save record to dummy event in bad partition]\n    # [fill in the fields as None or empty string]\n        event = [None, None, None, None, None, None, None, None, None, None, None, \"B\"]\n        return event", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 31.681884765625, "end_time": 1628656057157.916}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 5, "cell_type": "code", "source": "# Table schema for both csv and json files below. Note, that for both csv and json files, events with type Q or T may have \n# different item/events to categorize.\nschema = StructType([\n    StructField(\"trade_dt\", StringType(), True),\n    StructField(\"arrival_tm\", StringType(), True),\n    StructField(\"rec_type\", StringType(), True),\n    StructField(\"symbol\", StringType(), True),\n    StructField(\"event_tm\", StringType(), True),\n    StructField(\"event_seq_nb\", IntegerType(), True),\n    StructField(\"trade_pr/exch\", StringType(), True),\n    StructField(\"bid_pr\", DecimalType(), True),\n    StructField(\"bid_size\", IntegerType(), True),\n    StructField(\"ask_pr\", DecimalType(), True),\n    StructField(\"ask_size\", IntegerType(), True),\n    StructField(\"partition\", StringType(), True)\n])", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 33.56201171875, "end_time": 1628656057245.31}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 6, "cell_type": "code", "source": "# SparkContext represents the connection to a Spark cluster, and can be used to create RDDs, \n# accumulators and broadcast variables on that cluster. Only one SparkContext should be active per JVM. \n# You must stop() the active SparkContext before creating a new one.\n\n# To load the csv text files\ncsv1 =spark.sparkContext.textFile(\"wasbs://guidedcapstone@adfrcdevstorage.blob.core.windows.net/NYSE_csv.txt\")\ncsv2 =spark.sparkContext.textFile(\"wasbs://guidedcapstone@adfrcdevstorage.blob.core.windows.net/NYSE_8.06_csv.txt\")\n\n# To load the JSON text files\njson1 =spark.sparkContext.textFile(\"wasbs://guidedcapstone@adfrcdevstorage.blob.core.windows.net/Nasdaq_8.05_json.txt\")\njson2 =spark.sparkContext.textFile(\"wasbs://guidedcapstone@adfrcdevstorage.blob.core.windows.net/Nasdaq_8.06_json.txt\")", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 1341.8740234375, "end_time": 1628656058633.697}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 7, "cell_type": "code", "source": "# To implement the map/lambda function for each of the text files. Printing parsed# will only show\n# PythonRDD[26] at RDD at PythonRDD.scala:53\n\nparsed1 = csv1.map(lambda line: parse_csv(line))\n#print(parsed1.collect())\nparsed2 = csv2.map(lambda line: parse_csv(line))\n\n# We need to read the file as a JSON format BEFORE we pass it through the function\nparsed3 = json1.map(lambda line: parse_json(json.loads(line)))\n#print(parsed3.collect())\nparsed4 = json2.map(lambda line: parse_json(json.loads(line)))", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 243.0849609375, "end_time": 1628656058890.231}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 8, "cell_type": "code", "source": "# Create the dataframes\n\ncsv_8_05 = spark.createDataFrame(parsed1, schema)\ncsv_8_06 = spark.createDataFrame(parsed2, schema)\njson_8_05 = spark.createDataFrame(parsed3, schema)\njson_8_06 = spark.createDataFrame(parsed4, schema)\n\n# To test if the dataframe has been loaded with nonnull items\ncsv_8_05.limit(5).show()\njson_8_05.limit(5).show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+----------+--------------------+--------+------+--------------------+------------+-------------+------+--------+------+--------+---------+\n|  trade_dt|          arrival_tm|rec_type|symbol|            event_tm|event_seq_nb|trade_pr/exch|bid_pr|bid_size|ask_pr|ask_size|partition|\n+----------+--------------------+--------+------+--------------------+------------+-------------+------+--------+------+--------+---------+\n|2020-08-05|2020-08-05 09:30:...|       Q|  SYMA|2020-08-05 09:34:...|           1|         NYSE|    75|     100|    75|     100|        Q|\n|2020-08-05|2020-08-05 09:30:...|       Q|  SYMA|2020-08-05 09:40:...|           2|         NYSE|    77|     100|    79|     100|        Q|\n|2020-08-05|2020-08-05 09:30:...|       Q|  SYMA|2020-08-05 09:50:...|           3|         NYSE|    77|     100|    77|     100|        Q|\n|2020-08-05|2020-08-05 09:30:...|       Q|  SYMA|2020-08-05 09:57:...|           4|         NYSE|    79|     100|    80|     100|        Q|\n|2020-08-05|2020-08-05 09:30:...|       Q|  SYMA|2020-08-05 10:06:...|           5|         NYSE|    78|     100|    78|     100|        Q|\n+----------+--------------------+--------+------+--------------------+------------+-------------+------+--------+------+--------+---------+\n\n+----------+--------------------+--------+------+--------------------+------------+-------------+------+--------+------+--------+---------+\n|  trade_dt|          arrival_tm|rec_type|symbol|            event_tm|event_seq_nb|trade_pr/exch|bid_pr|bid_size|ask_pr|ask_size|partition|\n+----------+--------------------+--------+------+--------------------+------------+-------------+------+--------+------+--------+---------+\n|2020-08-05|2020-08-05 09:30:...|       Q|  SYMB|2020-08-05 15:33:...|          52|       NASDAQ|    34|     100|    36|     100|        Q|\n|2020-08-05|2020-08-05 09:30:...|       Q|  SYMB|2020-08-05 15:43:...|          53|       NASDAQ|    37|     100|    37|     100|        Q|\n|2020-08-05|2020-08-05 09:30:...|       Q|  SYMB|2020-08-05 15:50:...|          54|       NASDAQ|    33|     100|    35|     100|        Q|\n|2020-08-05|2020-08-05 09:30:...|       Q|  SYMB|2020-08-05 15:56:...|          55|       NASDAQ|    36|     100|    36|     100|        Q|\n|2020-08-05|2020-08-05 09:30:...|       Q|  SYMB|2020-08-05 16:05:...|          56|       NASDAQ|    37|     100|    37|     100|        Q|\n+----------+--------------------+--------+------+--------------------+------------+-------------+------+--------+------+--------+---------+"}], "metadata": {"cell_status": {"execute_time": {"duration": 13326.608154296875, "end_time": 1628656072230.469}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 10, "cell_type": "code", "source": "# Export this to the file destination to view whether or not it is processing correctly. Sorted in your HDIStorage container\n\ncsv_folder = 'wasbs://guidedcapstone-2021-08-09t17-05-44-969z@guidedcapstonhdistorage.blob.core.windows.net/HdiNotebooks/csv_folder'\njson_folder = 'wasbs://guidedcapstone-2021-08-09t17-05-44-969z@guidedcapstonhdistorage.blob.core.windows.net/HdiNotebooks/json_folder'\n\ncsv_8_05.write.partitionBy(\"partition\").mode(\"overwrite\").parquet(csv_folder)\ncsv_8_06.write.partitionBy(\"partition\").mode(\"overwrite\").parquet(csv_folder)\njson_8_05.write.partitionBy(\"partition\").mode(\"overwrite\").parquet(json_folder)\njson_8_06.write.partitionBy(\"partition\").mode(\"overwrite\").parquet(json_folder)\n", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 17503.294189453125, "end_time": 1628656119683.78}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true, "editable": true, "deletable": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark3", "name": "pyspark3kernel", "language": ""}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python3", "name": "pyspark3", "codemirror_mode": {"version": 3, "name": "python"}}, "anaconda-cloud": {}}}