# To Create a map/reduce function using Pyspark through the Hadoop platform

Please refer to the Spark_Project.py file for the data. Essentially runs as a map/reduce task through the use of Pyspark on Hadoop utilizing rdd's and the built in lambda function. Other useful functions include the groupByKey which acts as a mapper and the reduceByKey acting as the reducer.

The Spark_Project_CLI_Exec provides detailed information over the pyspark job run using the shell script in the terminal: 
## spark-submit Spark_Project.py
